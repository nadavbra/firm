{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Use whatever path is convenient for you\n",
    "CLINVAR_RAW_DATA_DIR = os.path.join(os.path.expanduser('~'), 'data/clinvar')\n",
    "\n",
    "# Change to the path where your project is cloned\n",
    "FIRM_DATA_DIR = os.path.join(os.path.expanduser('~'), 'github_projects/firm/firm/data')\n",
    "\n",
    "FINAL_DATASET_FEATURES_FILE_PATH = os.path.join(FIRM_DATA_DIR, 'clinvar_final_dataset_features.csv.gz')\n",
    "CLASSIFIER_DUMP_FILE_PATH = os.path.join(FIRM_DATA_DIR, 'classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(df, n = 5):\n",
    "    display(df.head(n))\n",
    "    print('%d records' % len(df))\n",
    "    \n",
    "def update_progress_bar(progress_bar, i, sensitivity):\n",
    "    if i % sensitivity == 0 or i == progress_bar.max:\n",
    "        progress_bar.value = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geneffect\n",
    "import firm\n",
    "\n",
    "REFERENCE_GENOME = 'GRCh37'\n",
    "\n",
    "geneffect_setup = geneffect.Setup(REFERENCE_GENOME)\n",
    "firm.setup_uniprot_tracks(geneffect_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare ClinVar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE: If you just want to load the pre-extracted features, you can skip this entire section. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download ClinVar's full dataset from their FTP website at:\n",
    "# ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz\n",
    "clinvar_data = pd.read_csv(os.path.join(CLINVAR_RAW_DATA_DIR, 'variant_summary.txt.gz'), delimiter = '\\t', \\\n",
    "        na_values = ['-', 'na'])\n",
    "summarize(clinvar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only SNPs with the correct reference genome\n",
    "clinvar_snps = clinvar_data.loc[(clinvar_data['Type'] == 'single nucleotide variant') & (clinvar_data['Assembly'] == \\\n",
    "        REFERENCE_GENOME), ['Chromosome', 'Start', 'ReferenceAllele', 'AlternateAllele', \\\n",
    "        'ClinicalSignificance']].copy().reset_index(drop = True)\n",
    "summarize(clinvar_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the variant processing framework to processes the SNPs\n",
    "\n",
    "total_failures = 0\n",
    "progress_bar = FloatProgress(min = 0, max = len(clinvar_snps) - 1)\n",
    "display(progress_bar)\n",
    "\n",
    "def process_snp_from_record(snp_record):\n",
    "    \n",
    "    update_progress_bar(progress_bar, snp_record.name, 1000)\n",
    "    \n",
    "    try:\n",
    "        return geneffect_setup.variant_interpreter.process_snp(str(snp_record['Chromosome']), int(snp_record['Start']), \\\n",
    "                snp_record['ReferenceAllele'], snp_record['AlternateAllele'])\n",
    "    except:\n",
    "        global total_failures\n",
    "        total_failures += 1\n",
    "        return np.nan\n",
    "            \n",
    "clinvar_snps['processed_snp'] = clinvar_snps.apply(process_snp_from_record, axis = 1)\n",
    "print('%d of %d failed.' % (total_failures, len(clinvar_snps)))\n",
    "summarize(clinvar_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter only missense variants\n",
    "\n",
    "def is_missense(snp):\n",
    "    return pd.notnull(snp) and len(snp.gene_effects) == 1 and snp.gene_effects[0].is_missense()\n",
    "\n",
    "clinvar_missense_snps = clinvar_snps.loc[clinvar_snps['processed_snp'].apply(is_missense)].copy()\n",
    "summarize(clinvar_missense_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine pathogenicity of the variants and drop duplicates\n",
    "\n",
    "SPLIT_REGEX = re.compile('[/;]')\n",
    "PATHOGENIC_KEYWORDS = set(['pathogenic', 'likely pathogenic'])\n",
    "BENIGN_KEYWORDS = set(['benign', 'likely benign'])\n",
    "\n",
    "def determine_pathogenicity(keywords):\n",
    "    if keywords.intersection(PATHOGENIC_KEYWORDS):\n",
    "        return 1\n",
    "    elif keywords.intersection(BENIGN_KEYWORDS):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def parse_clinical_significance(significance):\n",
    "    if pd.isnull(significance) or significance.isdigit():\n",
    "        return set()\n",
    "    else:\n",
    "        return set([keyword.lower() for keyword in SPLIT_REGEX.split(significance)])\n",
    "\n",
    "pathogenicity_keywords = clinvar_missense_snps['ClinicalSignificance'].apply(parse_clinical_significance)\n",
    "keyword_counter = Counter(chain.from_iterable(pathogenicity_keywords))\n",
    "print('There are %d unique Clinical Significance keywords: %s' % (len(keyword_counter), str(keyword_counter)))\n",
    "\n",
    "clinvar_missense_snps['is_pathogenic'] = pathogenicity_keywords.apply(determine_pathogenicity)\n",
    "clinvar_missense_snps['is_pathogenic'].value_counts(dropna = False, normalize = True).plot.pie(\\\n",
    "        figsize = (4, 4), autopct = '%.f%%')\n",
    "\n",
    "final_clinvar_dataset = clinvar_missense_snps.dropna(subset = ['is_pathogenic']).drop_duplicates(['Chromosome', 'Start', \\\n",
    "        'ReferenceAllele', 'AlternateAllele', 'is_pathogenic'])\n",
    "summarize(final_clinvar_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting features. This is a long process (~2 hours)...\n",
    "\n",
    "from firm.variant_feature_extraction import FeatureExtractionSetup, get_snp_effect_feature_extractor\n",
    "\n",
    "feature_extraction_setup = FeatureExtractionSetup(geneffect_setup)\n",
    "snp_effect_feature_extractor = get_snp_effect_feature_extractor(feature_extraction_setup)\n",
    "\n",
    "records = final_clinvar_dataset\n",
    "records_gene_effect = records['processed_snp'].apply(lambda snp: snp.gene_effects[0])\n",
    "\n",
    "%time features = snp_effect_feature_extractor.create_features_as_dataframe(records_gene_effect, show_progress_bar = True)\n",
    "features.insert(0, 'label', list(records['is_pathogenic']))\n",
    "features.insert(0, 'seq', list(records_gene_effect.apply(lambda gene_effect: \\\n",
    "        str(gene_effect.affected_gene.uniprot_record.seq))))\n",
    "features.insert(0, 'snp_effect', list(records_gene_effect))\n",
    "\n",
    "summarize(features, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Override the CSV file of the final dataset features with the ones you have just extracted.\n",
    "# WARNING: This will actually override the project's CSV file. Consider saving to another path if you don't want to do that.\n",
    "features.to_csv(FINAL_DATASET_FEATURES_FILE_PATH, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Or, just load the pre-extracted features.\n",
    "# WARNING: If you have already run the above cells, this will override everything!\n",
    "features = pd.read_csv(FINAL_DATASET_FEATURES_FILE_PATH)\n",
    "summarize(features, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_records = features.loc[:, 'protein_length':]\n",
    "feature_names = np.array(X_records.columns)\n",
    "X = X_records.as_matrix()\n",
    "y = features['label'].as_matrix()\n",
    "\n",
    "print(X.shape, X.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE: this will give training error (with overfitting), not test error! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from firm.ml.classification import predict_prob\n",
    "from firm.ml.metric_helper import get_formatted_scores\n",
    "\n",
    "firm_classifier = firm.load_classifier(geneffect_setup)\n",
    "\n",
    "X_selected = firm_classifier.feature_selection.transform(X)\n",
    "y_pred_prob = predict_prob(firm_classifier.model, X_selected)\n",
    "y_pred = (y_pred_prob >= firm_classifier.default_prob_threshold)\n",
    "print(get_formatted_scores(y, y_pred, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from firm.ml.cross_validation import cross_validate\n",
    "\n",
    "### Setup ###\n",
    "\n",
    "SEED = 7126\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "### Dataset statistics ###\n",
    "\n",
    "n = len(y)\n",
    "n_positive = sum(y)\n",
    "n_negative = n - n_positive\n",
    "f_positive = float(n_positive) / n\n",
    "f_negative = 1 - f_positive\n",
    "\n",
    "print('Dataset size: %d' % n)\n",
    "print('Label imbalance: %f' % f_positive)\n",
    "\n",
    "\n",
    "### Choosing feature selection method and model ###\n",
    "\n",
    "feature_selection = VarianceThreshold()\n",
    "model = RandomForestClassifier(n_estimators = 100, min_samples_split = 50, class_weight = 'balanced', n_jobs = -1, \\\n",
    "        random_state = SEED)\n",
    "\n",
    "print('Model: %s' % model)\n",
    "print('-' * 50)\n",
    "\n",
    "\n",
    "### Cross validation ###\n",
    "\n",
    "cross_validate(X, y, model, n_folds = 3, feature_selection = feature_selection, feature_names = feature_names, \\\n",
    "        report_removed_features = False, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump the trained classifier.\n",
    "# WARNING: This will actually override the project's classifier. Consider saving to another path if you don't want to do that.\n",
    "\n",
    "from firm.ml.classification import Classifier\n",
    "\n",
    "classifier = Classifier(get_snp_effect_feature_extractor, feature_extractor_creator_args = [feature_extraction_setup], \\\n",
    "        model = model, feature_selection = feature_selection)\n",
    "classifier.train_on_processed_data(X, y)\n",
    "\n",
    "with open(CLASSIFIER_DUMP_FILE_PATH, 'wb') as f:\n",
    "    classifier.dump(f)\n",
    "    \n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
